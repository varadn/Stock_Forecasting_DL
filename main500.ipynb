{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2a4fdd0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2a4fdd0f",
        "outputId": "d758fb02-8b42-4705-d4bd-bcaa6df69ea7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from copy import deepcopy as dc\n",
        "from torch.utils.data import DataLoader\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "file = '../../archive/YUM.csv'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "files = glob.glob(\"../../archive/*.csv\")\n",
        "# random.shuffle(files)\n",
        "# files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Training</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "iue5WvTxmVKB",
      "metadata": {
        "id": "iue5WvTxmVKB"
      },
      "outputs": [],
      "source": [
        "from Modules.train import train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1jgb1aJ3lxlQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jgb1aJ3lxlQ",
        "outputId": "2a25db36-4872-44e4-feca-4d26bad49f96"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load data into pytorch dataset\n",
        "\n",
        "from Modules.dataset_class import TimeSeriesDataset\n",
        "from Modules.preprocess import process_data\n",
        "def train_on_file(file, model, num_epochs, loss_function, optimizer, device):\n",
        "\n",
        "    print(\"PROCESSING \"+ file)\n",
        "    _, _, X_train, X_test, y_train, y_test, _ = process_data(file)\n",
        "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "    X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "\n",
        "    # create batches\n",
        "    batch_size = 16\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = train_model(num_epochs, model, loss_function, optimizer, train_loader,test_loader, device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_all(files, input_model, num_epochs, loss_function, optimizer, device):\n",
        "    model = input_model\n",
        "    save = 1\n",
        "    for file in files:\n",
        "        model = train_on_file(file, model, num_epochs, loss_function, optimizer, device)\n",
        "        if save % 5 == 0:\n",
        "            torch.save(model, 'checkpoints/forecast'+str(save)+'.pt')\n",
        "        save += 1\n",
        "\n",
        "    torch.save(model, '../..forecast_rnn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROCESSING ../../archive/A.csv\n",
            "Epoch: 1\n",
            "Batch 100, Loss: 0.0059067\n",
            "Batch 200, Loss: 0.0000627\n",
            "Batch 300, Loss: 0.0000472\n",
            "\n",
            "Val Loss: 0.0003691\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "Batch 100, Loss: 0.0000365\n",
            "Batch 200, Loss: 0.0000356\n",
            "Batch 300, Loss: 0.0000509\n",
            "\n",
            "Val Loss: 0.0001876\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "Batch 100, Loss: 0.0000408\n",
            "Batch 200, Loss: 0.0000549\n",
            "Batch 300, Loss: 0.0000592\n",
            "\n",
            "Val Loss: 0.0003917\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "Batch 100, Loss: 0.0000601\n",
            "Batch 200, Loss: 0.0000464\n",
            "Batch 300, Loss: 0.0000320\n",
            "\n",
            "Val Loss: 0.0002597\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "Batch 100, Loss: 0.0000371\n",
            "Batch 200, Loss: 0.0000394\n",
            "Batch 300, Loss: 0.0000417\n",
            "\n",
            "Val Loss: 0.0001689\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "Batch 100, Loss: 0.0000354\n",
            "Batch 200, Loss: 0.0000516\n",
            "Batch 300, Loss: 0.0000312\n",
            "\n",
            "Val Loss: 0.0001231\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "Batch 100, Loss: 0.0000284\n",
            "Batch 200, Loss: 0.0000354\n",
            "Batch 300, Loss: 0.0000465\n",
            "\n",
            "Val Loss: 0.0002626\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "Batch 100, Loss: 0.0000451\n",
            "Batch 200, Loss: 0.0000271\n",
            "Batch 300, Loss: 0.0000344\n",
            "\n",
            "Val Loss: 0.0003365\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "Batch 100, Loss: 0.0000317\n",
            "Batch 200, Loss: 0.0000346\n",
            "Batch 300, Loss: 0.0000503\n",
            "\n",
            "Val Loss: 0.0002395\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "Batch 100, Loss: 0.0000321\n",
            "Batch 200, Loss: 0.0000254\n",
            "Batch 300, Loss: 0.0000443\n",
            "\n",
            "Val Loss: 0.0001995\n",
            "***************************************************\n",
            "\n",
            "PROCESSING ../../archive/AAL.csv\n",
            "Epoch: 1\n",
            "Batch 100, Loss: 0.0001826\n",
            "Batch 200, Loss: 0.0001731\n",
            "\n",
            "Val Loss: 0.0002890\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "Batch 100, Loss: 0.0001983\n",
            "Batch 200, Loss: 0.0001708\n",
            "\n",
            "Val Loss: 0.0000744\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "Batch 100, Loss: 0.0001433\n",
            "Batch 200, Loss: 0.0001683\n",
            "\n",
            "Val Loss: 0.0001129\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "Batch 100, Loss: 0.0001553\n",
            "Batch 200, Loss: 0.0001179\n",
            "\n",
            "Val Loss: 0.0000729\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "Batch 100, Loss: 0.0001794\n",
            "Batch 200, Loss: 0.0001023\n",
            "\n",
            "Val Loss: 0.0000717\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "Batch 100, Loss: 0.0001384\n",
            "Batch 200, Loss: 0.0001679\n",
            "\n",
            "Val Loss: 0.0000879\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "Batch 100, Loss: 0.0001300\n",
            "Batch 200, Loss: 0.0001352\n",
            "\n",
            "Val Loss: 0.0000754\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "Batch 100, Loss: 0.0001090\n",
            "Batch 200, Loss: 0.0001387\n",
            "\n",
            "Val Loss: 0.0000939\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "Batch 100, Loss: 0.0001713\n",
            "Batch 200, Loss: 0.0001107\n",
            "\n",
            "Val Loss: 0.0001700\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "Batch 100, Loss: 0.0001328\n",
            "Batch 200, Loss: 0.0001224\n",
            "\n",
            "Val Loss: 0.0001216\n",
            "***************************************************\n",
            "\n",
            "PROCESSING ../../archive/AAP.csv\n",
            "Epoch: 1\n",
            "Batch 100, Loss: 0.0000557\n",
            "Batch 200, Loss: 0.0000514\n",
            "Batch 300, Loss: 0.0000436\n",
            "\n",
            "Val Loss: 0.0001812\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "Batch 100, Loss: 0.0000451\n",
            "Batch 200, Loss: 0.0000374\n",
            "Batch 300, Loss: 0.0000448\n",
            "\n",
            "Val Loss: 0.0001951\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "Batch 100, Loss: 0.0000380\n",
            "Batch 200, Loss: 0.0000464\n",
            "Batch 300, Loss: 0.0000498\n",
            "\n",
            "Val Loss: 0.0001479\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "Batch 100, Loss: 0.0000394\n",
            "Batch 200, Loss: 0.0000650\n",
            "Batch 300, Loss: 0.0000435\n",
            "\n",
            "Val Loss: 0.0001485\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "Batch 100, Loss: 0.0000400\n",
            "Batch 200, Loss: 0.0000555\n",
            "Batch 300, Loss: 0.0000491\n",
            "\n",
            "Val Loss: 0.0001482\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "Batch 100, Loss: 0.0000515\n",
            "Batch 200, Loss: 0.0000413\n",
            "Batch 300, Loss: 0.0000540\n",
            "\n",
            "Val Loss: 0.0006181\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "Batch 100, Loss: 0.0000728\n",
            "Batch 200, Loss: 0.0000530\n",
            "Batch 300, Loss: 0.0000556\n",
            "\n",
            "Val Loss: 0.0001533\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "Batch 100, Loss: 0.0000495\n",
            "Batch 200, Loss: 0.0000673\n",
            "Batch 300, Loss: 0.0000413\n",
            "\n",
            "Val Loss: 0.0001483\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "Batch 100, Loss: 0.0000554\n",
            "Batch 200, Loss: 0.0000372\n",
            "Batch 300, Loss: 0.0000503\n",
            "\n",
            "Val Loss: 0.0001757\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "Batch 100, Loss: 0.0000380\n",
            "Batch 200, Loss: 0.0000486\n",
            "Batch 300, Loss: 0.0000549\n",
            "\n",
            "Val Loss: 0.0002638\n",
            "***************************************************\n",
            "\n",
            "PROCESSING ../../archive/AAPL.csv\n",
            "Epoch: 1\n",
            "Batch 100, Loss: 0.0000052\n",
            "Batch 200, Loss: 0.0000028\n",
            "Batch 300, Loss: 0.0000033\n",
            "Batch 400, Loss: 0.0000017\n",
            "Batch 500, Loss: 0.0000019\n",
            "Batch 600, Loss: 0.0000027\n",
            "\n",
            "Val Loss: 0.0001284\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "Batch 100, Loss: 0.0000021\n",
            "Batch 200, Loss: 0.0000030\n",
            "Batch 300, Loss: 0.0000033\n",
            "Batch 400, Loss: 0.0000031\n",
            "Batch 500, Loss: 0.0000016\n",
            "Batch 600, Loss: 0.0000034\n",
            "\n",
            "Val Loss: 0.0002361\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "Batch 100, Loss: 0.0000034\n",
            "Batch 200, Loss: 0.0000021\n",
            "Batch 300, Loss: 0.0000023\n",
            "Batch 400, Loss: 0.0000019\n",
            "Batch 500, Loss: 0.0000029\n",
            "Batch 600, Loss: 0.0000072\n",
            "\n",
            "Val Loss: 0.0003069\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "Batch 100, Loss: 0.0000030\n",
            "Batch 200, Loss: 0.0000032\n",
            "Batch 300, Loss: 0.0000027\n",
            "Batch 400, Loss: 0.0000429\n",
            "Batch 500, Loss: 0.0000054\n",
            "Batch 600, Loss: 0.0000019\n",
            "\n",
            "Val Loss: 0.0002682\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "Batch 100, Loss: 0.0000022\n",
            "Batch 200, Loss: 0.0000026\n",
            "Batch 300, Loss: 0.0000063\n",
            "Batch 400, Loss: 0.0000023\n",
            "Batch 500, Loss: 0.0000019\n",
            "Batch 600, Loss: 0.0000070\n",
            "\n",
            "Val Loss: 0.0003087\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "Batch 100, Loss: 0.0000025\n",
            "Batch 200, Loss: 0.0000043\n",
            "Batch 300, Loss: 0.0000039\n",
            "Batch 400, Loss: 0.0000037\n",
            "Batch 500, Loss: 0.0000053\n",
            "Batch 600, Loss: 0.0000041\n",
            "\n",
            "Val Loss: 0.0003666\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "Batch 100, Loss: 0.0000046\n",
            "Batch 200, Loss: 0.0000075\n",
            "Batch 300, Loss: 0.0000021\n",
            "Batch 400, Loss: 0.0000033\n",
            "Batch 500, Loss: 0.0000055\n",
            "Batch 600, Loss: 0.0000042\n",
            "\n",
            "Val Loss: 0.0005458\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "Batch 100, Loss: 0.0000050\n",
            "Batch 200, Loss: 0.0000058\n",
            "Batch 300, Loss: 0.0000080\n",
            "Batch 400, Loss: 0.0000054\n",
            "Batch 500, Loss: 0.0000041\n",
            "Batch 600, Loss: 0.0000027\n",
            "\n",
            "Val Loss: 0.0001815\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/varad/Documents/2023fall/Deep Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m files:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(item \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_all(files, model, num_epochs, loss_function, optimizer, device)\n",
            "\u001b[1;32m/Users/varad/Documents/2023fall/Deep Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m save \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m train_on_file(file, model, num_epochs, loss_function, optimizer, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m save \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(model, \u001b[39m'\u001b[39m\u001b[39mcheckpoints/forecast\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(save)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32m/Users/varad/Documents/2023fall/Deep Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(num_epochs, model, loss_function, optimizer, train_loader,test_loader, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/Documents/2023fall/Deep Learning/final_project/github/Stock_Forecasting_DL/Modules/train.py:32\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, model, loss_function, optimizer, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m \u001b[39m#backward prop\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m batch_index \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m99\u001b[39m:  \u001b[39m# print every 100 batches\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from Modules.model import ElmanRNN\n",
        "\n",
        "model = ElmanRNN(30, 1, 64, 16, 1,)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "loss_function = nn.HuberLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "files = sorted(glob.glob(\"../../archive/*.csv\"))[:10]\n",
        "# files = glob.glob(\"../..archive/*.csv\")\n",
        "# random.shuffle(files)\n",
        "\n",
        "files_list_path = \"../../checkpoints/filenames.txt\"\n",
        "with open(files_list_path, 'w') as file:\n",
        "    # Write each element of the string array to the file\n",
        "    for item in files:\n",
        "        file.write(item + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "train_all(files, model, num_epochs, loss_function, optimizer, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
