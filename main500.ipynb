{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a4fdd0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2a4fdd0f",
        "outputId": "d758fb02-8b42-4705-d4bd-bcaa6df69ea7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from copy import deepcopy as dc\n",
        "from torch.utils.data import DataLoader\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "file = '../../archive/YUM.csv'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76b863ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "files = glob.glob(\"../../archive/*.csv\")\n",
        "# random.shuffle(files)\n",
        "# files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da84f129",
      "metadata": {},
      "source": [
        "<h2>Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021d459f",
      "metadata": {},
      "source": [
        "<h2>Training</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "iue5WvTxmVKB",
      "metadata": {
        "id": "iue5WvTxmVKB"
      },
      "outputs": [],
      "source": [
        "from Modules.train import train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1jgb1aJ3lxlQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jgb1aJ3lxlQ",
        "outputId": "2a25db36-4872-44e4-feca-4d26bad49f96"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load data into pytorch dataset\n",
        "\n",
        "from Modules.dataset_class import TimeSeriesDataset\n",
        "from Modules.preprocess import process_data\n",
        "def train_on_file(file, model, num_epochs, loss_function, optimizer, device):\n",
        "\n",
        "    print(\"PROCESSING \"+ file)\n",
        "    _, _, X_train, X_test, y_train, y_test, _ = process_data(file)\n",
        "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "    X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "\n",
        "    # create batches\n",
        "    batch_size = 16\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = train_model(num_epochs, model, loss_function, optimizer, train_loader,test_loader, device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "98656d3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_all(files, input_model, num_epochs, loss_function, optimizer, device):\n",
        "    model = input_model\n",
        "    save = 1\n",
        "    for file in files:\n",
        "        model = train_on_file(file, model, num_epochs, loss_function, optimizer, device)\n",
        "        if save % 5 == 0:\n",
        "            torch.save(model, 'checkpoints/forecast'+str(save)+'.pt')\n",
        "        save += 1\n",
        "\n",
        "    torch.save(model, 'forecast_final.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb280dcb",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/filenames.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/varad/Documents/2023fall/Deep Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m random\u001b[39m.\u001b[39mshuffle(files)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m files_list_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckpoints/filenames.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(files_list_path, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Write each element of the string array to the file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m files:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/varad/Documents/2023fall/Deep%20Learning/final_project/github/Stock_Forecasting_DL/main500.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(item \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/deep/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/filenames.txt'"
          ]
        }
      ],
      "source": [
        "\n",
        "from Modules.model import LSTM\n",
        "\n",
        "model = LSTM(30, 1, 64, 16, 1,)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "loss_function = nn.HuberLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# files = sorted(glob.glob(\"archive/*.csv\"))[:10]\n",
        "files = glob.glob(\"../..archive/*.csv\")\n",
        "random.shuffle(files)\n",
        "\n",
        "files_list_path = \"../../checkpoints/filenames.txt\"\n",
        "with open(files_list_path, 'w') as file:\n",
        "    # Write each element of the string array to the file\n",
        "    for item in files:\n",
        "        file.write(item + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "train_all(files, model, num_epochs, loss_function, optimizer, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
